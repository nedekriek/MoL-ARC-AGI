{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo running the Llama-3.2-3B-ARChitects-ReArc-bnb-4bit model \n",
    "\n",
    "### Prerequisites \n",
    "\n",
    "1. The `Llama-3.2-3B-ARChitects-ReArc-bnb-4bit` folder should be at the top level of this project and is imported as a git submodule when your ran `git submodule update --init`\n",
    "2. `Llama-3.2-3B-ARChitects-ReArc-bnb-4bit/model.safetensors` is a large file and is pulled not from github.com but huggingface.com. This means that you need to either setup an [ssh key](https://huggingface.co/docs/hub/security-git-ssh) or personal access token for your huggingface account. Once this is done you can pull `the model.safetensors` file by `cd Llama-3.2-3B-ARChitects-ReArc-bnb-4bit` followed by `git lfs pull`. Note git LFS should be  installed and initialised as per the *Getting Started* section in the main read me.\n",
    "3. All the required packages (except for `bitsandbytes non CUDA backend` ) are managed by uv. Run `uv sync` to make sure they are installed.\n",
    "4. Manually install `bitsandbytes non CUDA backend` with this [guide](https://huggingface.co/docs/bitsandbytes/main/en/installation?backend=Intel+CPU+%2B+GPU#multi-backend) by huggingface. Availability is hardware dependant, I suspect the mac users among us do not have their hardware supported - in this case we can move this repository onto a hosted platform with cloud compute.\n",
    "\n",
    "*Before starting this notebook always make sure you have done:* `uv pip install -e \"../bitsandbytes/\"`. *Otherwise do this now and restart the kernel.*\n",
    "\n",
    "##### Notes/recommendations:\n",
    "\n",
    "If you are compiling the `bitsandbytes`package with a non-CUDA backend from source. Clone the repo adjacent to this one and follow the build instructions. You can install the package to the venv associated with this repo via running `uv pip install -e \"../bitsandbytes/\"`in your terminal.\n",
    "\n",
    "In general you can prefix any `pip install` command with uv for the uv package manager to add packages installed this way to its dependency graph.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache import Cache\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Local methods\n",
    "from mol_arc_agi.io_helpers import load_all_json_files_concurrently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input paths\n",
    "# Path to model and tokenizer repository (submodule)   \n",
    "model_directory = Path('../../Llama-3.2-3B-ARChitects-ReArc-bnb-4bit')\n",
    "# Path to training data (original ARC data)\n",
    "training_data_directory = Path('../../ARC-AGI/data/training/')\n",
    "\n",
    "# Output paths\n",
    "base_output_directory = Path('output/')\n",
    "inference_cache = base_output_directory.joinpath('inference_cache/')\n",
    "submission = base_output_directory.joinpath('submission/')\n",
    "\n",
    "# Check paths are as expected\n",
    "print(model_directory.resolve()) \n",
    "print(training_data_directory.resolve())  \n",
    "print(base_output_directory.resolve())\n",
    "print(inference_cache.resolve())\n",
    "print(submission.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_all_json_files_concurrently(training_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "\n",
    "# Check if the tokenizer and model are loaded correctly\n",
    "assert tokenizer is not None, \"Tokenizer not loaded correctly.\"\n",
    "assert model is not None, \"Model not loaded correctly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set prompt formatting options\n",
    "prompt_fmt_opts = dict(\n",
    "    preprompt='ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjklmnpqrstuvwxyz',\n",
    "    query_begin='I',\n",
    "    reply_begin='\\n+/-=O',\n",
    "    reply_end='\\n' + tokenizer.eos_token,\n",
    "    lines_sep='\\n',\n",
    "    max_tokens=128000,\n",
    ")\n",
    "\n",
    "# set inference time data augmentation options\n",
    "inference_data_augmentation_opts = dict(tp='all', \n",
    "                                        rt='all',   #\n",
    "                                        perm=True,  #Permute the order of the tasks\n",
    "                                        shfl_ex=True, \n",
    "                                        seed=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the inference cache\n",
    "model_cache = Cache(inference_cache).memoize(typed=True, ignore=set(['model_tok', 'guess']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start refactor of the inference tools\n",
    "\n",
    "def dfs(model,              # formally the explore function\n",
    "        current_generated_tokens, # originally named path\n",
    "        gen_start_position, # start\n",
    "        max_gen_tokens,     # goal\n",
    "        min_probability,    # early stopping condition\n",
    "        cache, \n",
    "        current_score=0.0):\n",
    "    pass #https://huggingface.co/docs/transformers/v4.48.0/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "\n",
    "def init_dfs(model, \n",
    "        input_text_token_ids,\n",
    "        eos_token_id,\n",
    "        max_gen_tokens, #TODO: consider moving this\n",
    "        min_probability,\n",
    "        attention_mask=None):\n",
    "    \n",
    "    assert not torch.is_grad_enabled(), \"Gradient computation should be disabled.\"\n",
    "    assert attention_mask is None or attention_mask.all(), \"Attention mask not fully implemented.\"\n",
    "    \n",
    "    # Set recursion limit to avoid stack overflows\n",
    "    sys.setrecursionlimit(1000 + max_gen_tokens)\n",
    "\n",
    "    # prepare inputs\n",
    "    input = torch.as_tensor(input_text_token_ids, device=model.device, dtype=int)\n",
    "    if input.ndim == 2:\n",
    "        input = input.squeeze(0)\n",
    "    assert input.ndim == 1, 'Batching not supported.'\n",
    "\n",
    "    # Remove end of string token if it is in the input\n",
    "    if input[-1] == eos_token_id:\n",
    "        input = input[:-1]\n",
    "\n",
    "    gen_start_position = len(input)\n",
    "\n",
    "    # run DFS\n",
    "    result = explore(model, \n",
    "                     gen_start_position,\n",
    "                     max_gen_tokens,\n",
    "                     min_probability,\n",
    "                     cache)\n",
    "\n",
    "     # return results sorted by scores\n",
    "    return sorted([(np.array(suffix[::-1]), score_val) for suffix, score_val in result], key=lambda x: x[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
